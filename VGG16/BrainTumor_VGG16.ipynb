{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style='color:blue'>|</span> ğŸ§  Brain Tumor Detection using VGG16 </b> <hr>\n",
    "\n",
    "â€¢ A deep learning-based project to classify MRI brain images into Tumor / No Tumor using VGG16 architecture. \n",
    "# <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 1 <span style='color:red'>|</span> ğŸ“ Data Preparation and Folder Structure Creation </b> \n",
    "\n",
    "â€¢ This section defines the paths for the dataset and ensures that subfolders for each class (no, yes) exist in the training, validation, and testing directories.\n",
    "\n",
    "# <br>\n",
    "\n",
    "## <b> 2 <span style='color:red'>|</span> ğŸ”€ Split the Dataset into Train, Test, and Validation </b> \n",
    "\n",
    "â€¢ This function takes all images from a class, shuffles them, and splits them according to the defined ratios into train, test, and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Original data directory\n",
    "source_path = r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\"\n",
    "\n",
    "# Paths for training, testing, and validation sets\n",
    "train_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\TRAIN\")\n",
    "test_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\TEST\")\n",
    "val_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\VAL\")\n",
    "\n",
    "# Create class subfolders ('no', 'yes') inside TRAIN, TEST, and VAL if they don't exist\n",
    "for folder in [train_path, test_path, val_path]:\n",
    "    for cls in [\"no\", \"yes\"]:\n",
    "        os.makedirs(os.path.join(folder, cls), exist_ok=True)\n",
    "\n",
    "# Function to split images into TRAIN, TEST, and VAL sets\n",
    "def split_data(class_name, split_ratio=(0.7, 0.15, 0.15)):\n",
    "    class_path = os.path.join(source_path, class_name)  # Path to class folder (e.g., 'no' or 'yes')\n",
    "    images = os.listdir(class_path)  # List all images in the class folder\n",
    "    random.shuffle(images)  # Shuffle images randomly\n",
    "\n",
    "    # Calculate number of images for each split\n",
    "    train_size = int(len(images) * split_ratio[0])\n",
    "    test_size = int(len(images) * split_ratio[1])\n",
    "\n",
    "    # Split the images into train, test, and validation\n",
    "    train_images = images[:train_size]\n",
    "    test_images = images[train_size:train_size + test_size]\n",
    "    val_images = images[train_size + test_size:]\n",
    "\n",
    "    # Move images to their corresponding folders\n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(train_path, class_name, img))\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(test_path, class_name, img))\n",
    "    for img in val_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(val_path, class_name, img))\n",
    "\n",
    "# Split data for both classes\n",
    "split_data(\"no\")\n",
    "split_data(\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 3 <span style='color:red'>|</span> ğŸ§ª Data Generators (Augmentation & Preprocessing) </b> \n",
    "\n",
    "â€¢ This section prepares the images for training, validation, and testing using ImageDataGenerator.\n",
    "It applies data augmentation only for training to make the model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2100 images belonging to 2 classes.\n",
      "Found 450 images belonging to 2 classes.\n",
      "Found 450 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Paths to training, testing, and validation datasets\n",
    "train_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\TRAIN\")\n",
    "test_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\TEST\")\n",
    "val_path = os.path.join(source_path, r\"D:\\Projects\\Brain Tumor\\VGG16\\Data\\VAL\")\n",
    "\n",
    "# Define ImageDataGenerator for training with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,             # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,          # Randomly rotate images within 20 degrees\n",
    "    width_shift_range=0.2,      # Randomly shift images horizontally\n",
    "    height_shift_range=0.2,     # Randomly shift images vertically\n",
    "    horizontal_flip=True,       # Randomly flip images horizontally\n",
    "    fill_mode='nearest'         # Fill in missing pixels after transformation\n",
    ")\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),     # Resize images to 224x224 (suitable for VGG16)\n",
    "    batch_size=32,\n",
    "    class_mode='binary'         # Binary classification: tumor / no tumor\n",
    ")\n",
    "\n",
    "# Define ImageDataGenerator for validation (no augmentation)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create validation data generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Create test data generator (also no augmentation)\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 4 <span style='color:red'>|</span> ğŸ—ï¸ 4. Model Building using VGG16 </b> \n",
    "\n",
    "â€¢ Here, we load the VGG16 model pre-trained on ImageNet without the fully connected layers on top.\n",
    "We freeze all layers except the last two, and add custom layers for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      â”‚    \u001b[38;5;34m14,714,688\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚    \u001b[38;5;34m12,845,568\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m513\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,560,769</span> (105.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,560,769\u001b[0m (105.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,205,889</span> (58.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,205,889\u001b[0m (58.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,354,880</span> (47.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,354,880\u001b[0m (47.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# Load the VGG16 model without the top fully connected layers\n",
    "# Use pre-trained ImageNet weights and input shape suitable for our data\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers except the last two for fine-tuning\n",
    "for layer in base_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the final model\n",
    "model = Sequential([\n",
    "    base_model,   # Base VGG16 model\n",
    "    Flatten(),  # Flatten the output to feed into Dense layers\n",
    "    Dense(512, activation='relu'),  # Fully connected hidden layer with ReLU activation\n",
    "    Dropout(0.5),  # Dropout layer to reduce overfitting\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 5 <span style='color:red'>|</span> ğŸ‹ï¸â€â™‚ï¸ 5. Model Training with Early Stopping </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.12.8\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6251 - loss: 1.6956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.12.8\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 4s/step - accuracy: 0.6266 - loss: 1.6825 - val_accuracy: 0.8533 - val_loss: 0.3320\n",
      "Epoch 2/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 4s/step - accuracy: 0.8894 - loss: 0.2632 - val_accuracy: 0.8667 - val_loss: 0.2918\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 4s/step - accuracy: 0.9211 - loss: 0.2046 - val_accuracy: 0.9000 - val_loss: 0.2009\n",
      "Epoch 4/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 4s/step - accuracy: 0.9445 - loss: 0.1554 - val_accuracy: 0.9556 - val_loss: 0.1147\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 4s/step - accuracy: 0.9613 - loss: 0.1070 - val_accuracy: 0.9489 - val_loss: 0.1271\n",
      "Epoch 6/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9821 - loss: 0.0640 - val_accuracy: 0.9333 - val_loss: 0.1875\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9736 - loss: 0.0690 - val_accuracy: 0.9511 - val_loss: 0.1659\n",
      "Epoch 8/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9746 - loss: 0.0540 - val_accuracy: 0.9622 - val_loss: 0.1579\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 4s/step - accuracy: 0.9718 - loss: 0.0777 - val_accuracy: 0.9667 - val_loss: 0.0951\n",
      "Epoch 10/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9807 - loss: 0.0589 - val_accuracy: 0.9800 - val_loss: 0.0761\n",
      "Epoch 11/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 4s/step - accuracy: 0.9907 - loss: 0.0313 - val_accuracy: 0.9689 - val_loss: 0.1249\n",
      "Epoch 12/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9889 - loss: 0.0303 - val_accuracy: 0.9422 - val_loss: 0.2281\n",
      "Epoch 13/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 4s/step - accuracy: 0.9703 - loss: 0.0981 - val_accuracy: 0.9711 - val_loss: 0.0721\n",
      "Epoch 14/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 4s/step - accuracy: 0.9839 - loss: 0.0373 - val_accuracy: 0.9756 - val_loss: 0.0974\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9864 - loss: 0.0361 - val_accuracy: 0.9844 - val_loss: 0.0554\n",
      "Epoch 16/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - accuracy: 0.9878 - loss: 0.0315 - val_accuracy: 0.9467 - val_loss: 0.1577\n",
      "Epoch 17/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 4s/step - accuracy: 0.9880 - loss: 0.0423 - val_accuracy: 0.9800 - val_loss: 0.0761\n",
      "Epoch 18/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 4s/step - accuracy: 0.9939 - loss: 0.0206 - val_accuracy: 0.9600 - val_loss: 0.1100\n",
      "Epoch 19/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 4s/step - accuracy: 0.9852 - loss: 0.0355 - val_accuracy: 0.9622 - val_loss: 0.1310\n",
      "Epoch 20/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 4s/step - accuracy: 0.9866 - loss: 0.0351 - val_accuracy: 0.9867 - val_loss: 0.0637\n",
      "âœ… Training completed!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Add early stopping to prevent overfitting\n",
    "# It stops training if the validation loss doesn't improve for 5 consecutive epochs\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # Monitor validation loss\n",
    "    patience=5,                   # Wait 5 epochs before stopping if no improvement\n",
    "    restore_best_weights=True     # Restore the best model weights after stopping\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,              # Training data\n",
    "    epochs=20,                    # Number of epochs (can be adjusted)\n",
    "    validation_data=val_generator,  # Validation data\n",
    "    callbacks=[early_stopping]   # Apply early stopping during training\n",
    ")\n",
    "\n",
    "print(\"âœ… Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 6 <span style='color:red'>|</span> ğŸ§ª Model Evaluation on Test Dataset </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.9777 - loss: 0.1003\n",
      "ğŸ”¥ Model Accuracy On Test Data: 97.56%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "# Print the model's accuracy on test data\n",
    "print(f\"ğŸ”¥ Model Accuracy On Test Data: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 7 <span style='color:red'>|</span> ğŸ’¾ 7. Save the Trained Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to an HDF5 file\n",
    "model.save(\"brain_tumor_vgg16.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
